{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nvme/node02/pranav/AE24/AudioLDM-training-finetuning/audioldm_train/utilities/audio/stft.py:42: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = pad_center(fft_window, filter_length)\n",
      "/mnt/nvme/node02/pranav/AE24/AudioLDM-training-finetuning/audioldm_train/utilities/audio/stft.py:145: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = librosa_mel_fn(\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from config import Config_MBM_EEG\n",
    "from mae_for_eeg import *\n",
    "from eeg_dataset import EEGDataset\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "configs= yaml.load(open(\"/mnt/nvme/node02/pranav/AE24/AudioLDM-training-finetuning/audioldm_train/config/2023_08_23_reproduce_audioldm/audioldm_eeg.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "dataset = EEGDataset(configs,split=\"train\")\n",
    "loader = DataLoader(dataset, shuffle=False,batch_size =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt= \"/mnt/nvme/node02/pranav/AE24/DreamDiffusion/dreamdiffusion/results/eeg_pretrain/21-11-2023-05-58-07/checkpoints/checkpoint.pth\"\n",
    "\n",
    "lopi=torch.load(ckpt,map_location='cpu')\n",
    "\n",
    "config =Config_MBM_EEG()\n",
    "model = MAEforEEG(time_len=512, patch_size=config.patch_size, embed_dim=config.embed_dim,\n",
    "                decoder_embed_dim=config.decoder_embed_dim, depth=config.depth, \n",
    "                num_heads=config.num_heads, decoder_num_heads=config.decoder_num_heads, mlp_ratio=config.mlp_ratio,\n",
    "                focus_range=config.focus_range, focus_rate=config.focus_rate, \n",
    "                img_recon_weight=config.img_recon_weight, use_nature_img_loss=config.use_nature_img_loss)  \n",
    "model.load_state_dict(lopi[\"model\"],strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from audioldm_train.modules.clap.open_clip import create_model\n",
    "from audioldm_train.modules.clap.training.data import get_audio_features\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from config import Config_MBM_EEG\n",
    "from mae_for_eeg import *\n",
    "from eeg_dataset import EEGDataset\n",
    "# from audioldm_train.utilities.data.dataset import AudioDataset\n",
    "\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG2Latent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEG2Latent,self).__init__()\n",
    "\n",
    "\n",
    "        ckpt= \"/mnt/nvme/node02/pranav/AE24/DreamDiffusion/dreamdiffusion/results/eeg_pretrain/21-11-2023-05-58-07/checkpoints/checkpoint.pth\"\n",
    "\n",
    "        state_dict=torch.load(ckpt,map_location='cpu')\n",
    "        config =Config_MBM_EEG()\n",
    "\n",
    "        self.eeg_encoder = eeg_encoder(512,4,512,64,24,16,1)\n",
    "        self.eeg_encoder.load_state_dict(state_dict,strict=False)\n",
    "\n",
    "        for param in self.eeg_encoder.parameters():\n",
    "            param.requires_grad =False\n",
    "        \n",
    "\n",
    "        self.eeg_projection = nn.Sequential(\n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(128*512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.eeg_encoder.forward(x.permute(0,2,1))\n",
    "        x=self.eeg_projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    AutoTokenizer,\n",
    "    T5EncoderModel,\n",
    "    MT5EncoderModel,\n",
    ")\n",
    "\n",
    "\n",
    "class CLAPAudioEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_path=\"/mnt/nvme/node02/pranav/AE24/AudioLDM-training-finetuning/data/checkpoints/clap_music_speech_audioset_epoch_15_esc_89.98.pt\",\n",
    "        sampling_rate=16000,\n",
    "        embed_mode=\"audio\",\n",
    "        amodel=\"HTSAT-base\",\n",
    "        unconditional_prob=0.1,\n",
    "        random_mute=False,\n",
    "        max_random_mute_portion=0.5,\n",
    "        training_mode=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = \"cpu\"\n",
    "        self.precision = \"fp32\"\n",
    "        self.amodel = amodel  # or 'PANN-14'\n",
    "        self.tmodel = \"roberta\"  # the best text encoder in our training\n",
    "        self.enable_fusion = False  # False if you do not want to use the fusion model\n",
    "        self.fusion_type = \"aff_2d\"\n",
    "        self.pretrained = pretrained_path\n",
    "        self.embed_mode = embed_mode\n",
    "        self.embed_mode_orig = embed_mode\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.unconditional_prob = unconditional_prob\n",
    "        self.random_mute = random_mute\n",
    "        self.tokenize = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "        self.max_random_mute_portion = max_random_mute_portion\n",
    "        self.training_mode = training_mode\n",
    "        self.model, self.model_cfg = create_model(\n",
    "            self.amodel,\n",
    "            self.tmodel,\n",
    "            self.pretrained,\n",
    "            precision=self.precision,\n",
    "            device=self.device,\n",
    "            enable_fusion=self.enable_fusion,\n",
    "            fusion_type=self.fusion_type,\n",
    "        )\n",
    "        audio_cfg = self.model_cfg[\"audio_cfg\"]\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=audio_cfg[\"sample_rate\"],\n",
    "            n_fft=audio_cfg[\"window_size\"],\n",
    "            win_length=audio_cfg[\"window_size\"],\n",
    "            hop_length=audio_cfg[\"hop_size\"],\n",
    "            center=True,\n",
    "            pad_mode=\"reflect\",\n",
    "            power=2.0,\n",
    "            norm=None,\n",
    "            onesided=True,\n",
    "            n_mels=64,\n",
    "            f_min=audio_cfg[\"fmin\"],\n",
    "            f_max=audio_cfg[\"fmax\"],\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.unconditional_token = None\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_unconditional_condition(self, batchsize):\n",
    "        self.unconditional_token = self.model.get_text_embedding(\n",
    "            self.tokenizer([\"\", \"\"])\n",
    "        )[0:1]\n",
    "        return torch.cat([self.unconditional_token.unsqueeze(0)] * batchsize, dim=0)\n",
    "\n",
    "    def batch_to_list(self, batch):\n",
    "        ret = []\n",
    "        for i in range(batch.size(0)):\n",
    "            ret.append(batch[i])\n",
    "        return ret\n",
    "\n",
    "    def make_decision(self, probability):\n",
    "        if float(torch.rand(1)) < probability:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def random_uniform(self, start, end):\n",
    "        val = torch.rand(1).item()\n",
    "        return start + (end - start) * val\n",
    "\n",
    "    def _random_mute(self, waveform):\n",
    "        # waveform: [bs, t-steps]\n",
    "        t_steps = waveform.size(-1)\n",
    "        for i in range(waveform.size(0)):\n",
    "            mute_size = int(\n",
    "                self.random_uniform(0, end=int(t_steps * self.max_random_mute_portion))\n",
    "            )\n",
    "            mute_start = int(self.random_uniform(0, t_steps - mute_size))\n",
    "            waveform[i, mute_start : mute_start + mute_size] = 0\n",
    "        return waveform\n",
    "\n",
    "    def cos_similarity(self, waveform, text):\n",
    "        # waveform: [bs, t_steps]\n",
    "        original_embed_mode = self.embed_mode\n",
    "        with torch.no_grad():\n",
    "            self.embed_mode = \"audio\"\n",
    "            audio_emb = self(waveform.cuda())\n",
    "            self.embed_mode = \"text\"\n",
    "            text_emb = self(text)\n",
    "            similarity = F.cosine_similarity(audio_emb, text_emb, dim=2)\n",
    "        self.embed_mode = original_embed_mode\n",
    "        return similarity.squeeze()\n",
    "\n",
    "    def build_unconditional_emb(self):\n",
    "        self.unconditional_token = self.model.get_text_embedding(\n",
    "            self.tokenizer([\"\", \"\"])\n",
    "        )[0:1]\n",
    "    def tokenizer(self, text):\n",
    "        result = self.tokenize(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in result.items()}\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # If you want this conditioner to be unconditional, set self.unconditional_prob = 1.0\n",
    "        # If you want this conditioner to be fully conditional, set self.unconditional_prob = 0.0\n",
    "        if self.model.training == True and not self.training_mode:\n",
    "            print(\n",
    "                \"The pretrained CLAP model should always be in eval mode. Reloading model just in case you change the parameters.\"\n",
    "            )\n",
    "            self.model, self.model_cfg = create_model(\n",
    "                self.amodel,\n",
    "                self.tmodel,\n",
    "                self.pretrained,\n",
    "                precision=self.precision,\n",
    "                device=\"cuda\",\n",
    "                enable_fusion=self.enable_fusion,\n",
    "                fusion_type=self.fusion_type,\n",
    "            )\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "            self.model.eval()\n",
    "\n",
    "        # if self.unconditional_token is None:\n",
    "        #     self.build_unconditional_emb()\n",
    "\n",
    "        # if(self.training_mode):\n",
    "        #     assert self.model.training == True\n",
    "        # else:\n",
    "        #     assert self.model.training == False\n",
    "\n",
    "        # the 'fusion' truncate mode can be changed to 'rand_trunc' if run in unfusion mode\n",
    "        if self.embed_mode == \"audio\":\n",
    "            # if not self.training:\n",
    "            #     print(\"INFO: clap model calculate the audio embedding as condition\")\n",
    "            with torch.no_grad():\n",
    "                # assert (\n",
    "                #     self.sampling_rate == 16000\n",
    "                # ), \"We only support 16000 sampling rate\"\n",
    "\n",
    "                # if self.random_mute:\n",
    "                #     batch = self._random_mute(batch)\n",
    "                # batch: [bs, 1, t-samples]\n",
    "                if self.sampling_rate != 48000:\n",
    "                    batch = torchaudio.functional.resample(\n",
    "                        batch, orig_freq=self.sampling_rate, new_freq=48000\n",
    "                    )\n",
    "\n",
    "                audio_data = batch.squeeze(1)\n",
    "                mel = self.mel_transform(audio_data)\n",
    "                audio_dict = get_audio_features(\n",
    "                    audio_data,\n",
    "                    mel,\n",
    "                    380000,\n",
    "                    data_truncating=\"fusion\",\n",
    "                    data_filling=\"repeatpad\",\n",
    "                    audio_cfg=self.model_cfg[\"audio_cfg\"],\n",
    "                )\n",
    "                \n",
    "                # [bs, 512]\n",
    "                embed = self.model.get_audio_embedding(audio_dict)\n",
    "        elif self.embed_mode == \"text\":\n",
    "            with torch.no_grad():\n",
    "                # the 'fusion' truncate mode can be changed to 'rand_trunc' if run in unfusion mode\n",
    "                text_data = self.tokenizer(batch)\n",
    "\n",
    "                if isinstance(batch, str) or (\n",
    "                    isinstance(batch, list) and len(batch) == 1\n",
    "                ):\n",
    "                    for key in text_data.keys():\n",
    "                        text_data[key] = text_data[key].unsqueeze(0)\n",
    "\n",
    "                embed = self.model.get_text_embedding(text_data)\n",
    "\n",
    "        embed = embed.unsqueeze(1)\n",
    "        # for i in range(embed.size(0)):\n",
    "        #     if self.make_decision(self.unconditional_prob):\n",
    "        #         embed[i] = self.unconditional_token\n",
    "        # embed = torch.randn((batch.size(0), 1, 512)).type_as(batch)\n",
    "        return embed.detach()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nvme/node02/pranav/AE24/AudioLDM-training-finetuning/audioldm_train/utilities/audio/stft.py:42: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = pad_center(fft_window, filter_length)\n",
      "/mnt/nvme/node02/pranav/AE24/AudioLDM-training-finetuning/audioldm_train/utilities/audio/stft.py:145: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel_basis = librosa_mel_fn(\n"
     ]
    }
   ],
   "source": [
    "configs= yaml.load(open(\"/mnt/nvme/node02/pranav/AE24/AudioLDM-training-finetuning/audioldm_train/config/2023_08_23_reproduce_audioldm/audioldm_eeg.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "dataset = EEGDataset(configs,split=\"train\")\n",
    "loader = DataLoader(dataset, shuffle=False,batch_size =100)\n",
    "# from audioldm_train.train.load_audio import audioDa\n",
    "# auloader = DataLoader(audioDa, shuffle=False,batch_size =10)\n",
    "# audiodata=next(iter(auloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.distributed as dist\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# dist.init_process_group(backend='nccl')\n",
    "checkpoint_path=\"/mnt/nvme/node02/pranav/AE24/AudioLDM-training-finetuning/audioldm_train/modules/eeg_encoder/models/eegLatentmodel_test.pth\"\n",
    "checkpoint_path=\"/mnt/nvme/node02/pranav/AE24/starDuck/CAPE/models/eegLatentmodel_Linfonce.pth\"\n",
    "eeg_model= EEG2Latent()\n",
    "eegmodel= torch.load(checkpoint_path,map_location=torch.device('cpu'))\n",
    "eeg_model.load_state_dict(eegmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/miniconda3/envs/ae24/lib/python3.10/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "/home/pranav/miniconda3/envs/ae24/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/883 [00:00<?, ?it/s]/mnt/nvme/node02/pranav/AE24/starDuck/CAPE/eeg_dataset.py:252: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = librosa_mel_fn(\n",
      "  0%|          | 0/883 [00:14<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "audio_model = CLAPAudioEmbedding()\n",
    "audio_model.eval()\n",
    "audio_model.embed_mode=\"audio\"\n",
    "eeg_model= EEG2Latent()\n",
    "\n",
    "optimizer = optim.Adam(eeg_model.parameters(), lr=0.001)\n",
    "cosine = nn.CosineSimilarity()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "numEpochs=1\n",
    "for epoch in range(numEpochs):\n",
    "    sum_loss=0\n",
    "    for batch in tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "        eeglatent = eeg_model(batch[\"eeg\"])\n",
    "        with torch.no_grad():\n",
    "            audiolatent = torch.squeeze(audio_model(batch[\"waveform\"]))\n",
    "\n",
    "        loss = (1- cosine(eeglatent,audiolatent)).mean()\n",
    "        sum_loss+=loss\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Training loss: {sum_loss/len(loader)} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 655, 64]), torch.Size([10, 1, 163750]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audiodata[\"eeg\"].shape,audiodata[\"waveform\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audiolatent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cosine(\u001b[43maudiolatent\u001b[49m,eeglatent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audiolatent' is not defined"
     ]
    }
   ],
   "source": [
    "cosine(audiolatent,eeglatent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nvme/node02/pranav/AE24/starDuck/CAPE/eeg_dataset.py:252: FutureWarning: Pass sr=16000, n_fft=1024, n_mels=64, fmin=0, fmax=8000 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mel = librosa_mel_fn(\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset, shuffle=False,batch_size =10)\n",
    "audiodata = next(iter(loader))\n",
    "ee = eeg_model(audiodata[\"eeg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*15/60\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audiodata[\"eeg\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav/miniconda3/envs/ae24/lib/python3.10/site-packages/torchlibrosa/stft.py:193: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = librosa.util.pad_center(fft_window, n_fft)\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "audio_model = CLAPAudioEmbedding()\n",
    "audio_model.embed_mode=\"audio\"\n",
    "ae = torch.squeeze(audio_model(audiodata[\"waveform\"]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 512]), torch.Size([10, 512]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "audio_model.embed_mode=\"text\"\n",
    "te = torch.squeeze(audio_model(audiodata[\"text\"]),dim=1)\n",
    "ae.shape,te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1998,  0.2578,  0.2446,  0.2426,  0.1279,  0.0878, -0.0385,  0.0282,\n",
       "         0.1468,  0.1716])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine = nn.CosineSimilarity()\n",
    "\n",
    "cosine(ae,te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "result=ae@te.T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "normalized_result = (result - torch.min(result)) / (torch.max(result) - torch.min(result))\n",
    "\n",
    "# Plotting the result as a color matrix\n",
    "io=result.numpy()\n",
    "io.shape\n",
    "\n",
    "plt.imshow(io,cmap=\"jet\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ae24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
